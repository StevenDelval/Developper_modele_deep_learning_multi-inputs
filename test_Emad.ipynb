{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import BERT tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 14:47:31.803910: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 14:47:31.991247: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 14:47:31.992762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-26 14:47:33.003904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/preparation.csv\")\n",
    "df = df[['Description'\t,'product_category_tree_target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>product_category_tree_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>Baby Care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  Key Features of Elegance Polyester Multicolor ...   \n",
       "1  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "2  Key Features of Eurospa Cotton Terry Face Towe...   \n",
       "\n",
       "  product_category_tree_target  \n",
       "0              Home Furnishing  \n",
       "1                    Baby Care  \n",
       "2                    Baby Care  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ratio for the split\n",
    "split_ratio = 0.8  # 80% training, 20% testing\n",
    "\n",
    "# Calculate the number of rows for training and testing\n",
    "total_rows = df.shape[0]\n",
    "train_rows = int(split_ratio * total_rows)\n",
    "test_rows = total_rows - train_rows\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_data = df.iloc[:train_rows, :]\n",
    "test_data = df.iloc[train_rows:, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label = preprocessing.LabelEncoder()\n",
    "y = label.fit_transform(train_data['product_category_tree_target'])\n",
    "y = to_categorical(y)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a BERT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(m_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/67043468/unparsedflagaccesserror-trying-to-access-flag-preserve-unused-tokens-before-f\n",
    "# Set the preserve_unused_tokens flag to True\n",
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len-len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Baby Care' 'Beauty and Personal Care' 'Computers'\n",
      " 'Home Decor & Festive Needs' 'Home Furnishing' 'Kitchen & Dining'\n",
      " 'Watches']\n"
     ]
    }
   ],
   "source": [
    "labels = label.classes_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cat = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    \n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
    "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    out = tf.keras.layers.Dense(len_cat, activation='softmax')(lay)  # len_cart = N de cat.\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "train_input = bert_encode(train_data.Description.values, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test_data.Description.values, tokenizer, max_len=max_len)\n",
    "train_labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer  [(None, 250)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)     [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)    [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)    [(None, 768),                1094822   ['input_word_ids[0][0]',      \n",
      "                              (None, 250, 768)]           41         'input_mask[0][0]',          \n",
      "                                                                     'segment_ids[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['keras_layer[0][1]']         \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   49216     ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   2080      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 32)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 7)                    231       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109533768 (417.84 MB)\n",
      "Trainable params: 109533767 (417.84 MB)\n",
      "Non-trainable params: 1 (1.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.1019 - accuracy: 0.1429 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.13095, saving model to model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apprenant/Documents/Developper_modele_deep_learning_multi-inputs/myenv/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 672s 31s/step - loss: 2.1019 - accuracy: 0.1429 - val_loss: 2.0335 - val_accuracy: 0.1310\n",
      "Epoch 2/3\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9328 - accuracy: 0.1592 \n",
      "Epoch 2: val_accuracy did not improve from 0.13095\n",
      "21/21 [==============================] - 680s 33s/step - loss: 1.9328 - accuracy: 0.1592 - val_loss: 2.0494 - val_accuracy: 0.0952\n",
      "Epoch 3/3\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9478 - accuracy: 0.1994 \n",
      "Epoch 3: val_accuracy did not improve from 0.13095\n",
      "21/21 [==============================] - 657s 31s/step - loss: 1.9478 - accuracy: 0.1994 - val_loss: 1.9589 - val_accuracy: 0.0655\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_sh = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    \n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "times",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
